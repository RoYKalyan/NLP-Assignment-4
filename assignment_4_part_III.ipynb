{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/M3_Assignment/blob/main/scripts/m3_assignment_part_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III\n",
        "Using the previous two tutorials, please answer the following using an encorder-decoder approach and an LSTM compared approach.\n",
        "\n",
        "Please create a transformer-based classifier for English name classification into male or female.\n",
        "\n",
        "There are several datasets for name for male or female classification. In subseuqent iterations, this could be expanded to included more classifications.\n",
        "\n",
        "Below is the source from NLTK, which only has male and female available but could be used for the purposes of this assignment.\n",
        "\n",
        "```\n",
        "names = nltk.corpus.names\n",
        "names.fileids()\n",
        "['female.txt', 'male.txt']\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "[w for w in male_names if w in female_names]\n",
        "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
        "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
        "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
        "```"
      ],
      "metadata": {
        "id": "QD5ia2HsZpsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 . Encode-Decoder Based Approach"
      ],
      "metadata": {
        "id": "A6VQZUDhzhTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Data Preparation and Model Setup"
      ],
      "metadata": {
        "id": "0fow93dhzpPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, MultiHeadAttention, LayerNormalization, Dense, GlobalAveragePooling1D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Download and prepare dataset\n",
        "nltk.download('names')\n",
        "from nltk.corpus import names\n",
        "\n",
        "# Load names\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "\n",
        "# Create labels\n",
        "male_labels = [0] * len(male_names)  # 0 for male\n",
        "female_labels = [1] * len(female_names)  # 1 for female\n",
        "\n",
        "# Combine datasets\n",
        "all_names = np.array(male_names + female_names)\n",
        "all_labels = np.array(male_labels + female_labels)\n",
        "\n",
        "# Shuffle dataset\n",
        "indices = np.arange(all_labels.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "all_names = all_names[indices]\n",
        "all_labels = all_labels[indices]\n",
        "\n",
        "# Text Vectorization\n",
        "vectorizer = TextVectorization(max_tokens=10000, output_sequence_length=10)\n",
        "vectorizer.adapt(all_names)\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorizer(text), label\n",
        "\n",
        "# Prepare the final datasets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((all_names, all_labels))\n",
        "dataset = dataset.map(vectorize_text)\n",
        "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC7iGarIvugN",
        "outputId": "20f8e890-75a6-4763-de44-50b7b49badaa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Encoder, Decoder, Model Building, and Training"
      ],
      "metadata": {
        "id": "_CknhT130FEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, MultiHeadAttention, LayerNormalization, Dense, GlobalAveragePooling1D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Download and prepare dataset\n",
        "nltk.download('names')\n",
        "from nltk.corpus import names\n",
        "\n",
        "# Load names\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "\n",
        "# Create labels\n",
        "male_labels = [0] * len(male_names)  # 0 for male\n",
        "female_labels = [1] * len(female_names)  # 1 for female\n",
        "\n",
        "# Combine datasets\n",
        "all_names = np.array(male_names + female_names)\n",
        "all_labels = np.array(male_labels + female_labels)\n",
        "\n",
        "# Shuffle dataset\n",
        "indices = np.arange(all_labels.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "all_names = all_names[indices]\n",
        "all_labels = all_labels[indices]\n",
        "\n",
        "# Text Vectorization\n",
        "vectorizer = TextVectorization(max_tokens=10000, output_sequence_length=10)\n",
        "vectorizer.adapt(all_names)\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "    text = vectorizer(text)  # Remove tf.expand_dims\n",
        "    return text, label\n",
        "\n",
        "# Prepare the final datasets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((all_names, all_labels))\n",
        "dataset = dataset.map(vectorize_text)\n",
        "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Define the Encoder\n",
        "def encoder(inputs, num_heads, ff_dim):\n",
        "    # Multi-head self-attention\n",
        "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "    # Skip connection and layer normalization\n",
        "    x = LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
        "    # Feed-forward network\n",
        "    ff_output = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    # Second skip connection and layer normalization\n",
        "    encoded_seq = LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
        "    return encoded_seq\n",
        "\n",
        "# Define the Decoder\n",
        "def decoder(encoded_seq):\n",
        "    x = GlobalAveragePooling1D()(encoded_seq)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
        "    return outputs\n",
        "\n",
        "# Building the Model\n",
        "inputs = Input(shape=(None,))\n",
        "x = Embedding(input_dim=10000, output_dim=64)(inputs)\n",
        "encoded_seq = encoder(x, num_heads=2, ff_dim=64)\n",
        "outputs = decoder(encoded_seq)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n",
        "\n",
        "# Train the Model\n",
        "model.fit(dataset, epochs=10)\n",
        "\n",
        "# Save the model weights\n",
        "model.save_weights('name_gender_classifier_weights.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms21Ptn6wBWP",
        "outputId": "dcdaf88d-f230-4ecb-c6b7-b6fe4a5cdbcf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, None, 64)             640000    ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, None, 64)             33216     ['embedding_2[0][0]',         \n",
            " ltiHeadAttention)                                                   'embedding_2[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TF  (None, None, 64)             0         ['embedding_2[0][0]',         \n",
            " OpLambda)                                                           'multi_head_attention_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, None, 64)             128       ['tf.__operators__.add_4[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, None, 64)             4160      ['layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, None, 64)             4160      ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TF  (None, None, 64)             0         ['layer_normalization_4[0][0]'\n",
            " OpLambda)                                                          , 'dense_9[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, None, 64)             128       ['tf.__operators__.add_5[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_2  (None, 64)                   0         ['layer_normalization_5[0][0]'\n",
            "  (GlobalAveragePooling1D)                                          ]                             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 20)                   1300      ['global_average_pooling1d_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 1)                    21        ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 683113 (2.61 MB)\n",
            "Trainable params: 683113 (2.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "249/249 [==============================] - 10s 13ms/step - loss: 0.6717 - accuracy: 0.6229\n",
            "Epoch 2/10\n",
            "249/249 [==============================] - 3s 12ms/step - loss: 0.4147 - accuracy: 0.8104\n",
            "Epoch 3/10\n",
            "249/249 [==============================] - 3s 12ms/step - loss: 0.1602 - accuracy: 0.9489\n",
            "Epoch 4/10\n",
            "249/249 [==============================] - 3s 11ms/step - loss: 0.0983 - accuracy: 0.9700\n",
            "Epoch 5/10\n",
            "249/249 [==============================] - 3s 11ms/step - loss: 0.0716 - accuracy: 0.9783\n",
            "Epoch 6/10\n",
            "249/249 [==============================] - 3s 14ms/step - loss: 0.0946 - accuracy: 0.9697\n",
            "Epoch 7/10\n",
            "249/249 [==============================] - 4s 15ms/step - loss: 0.0794 - accuracy: 0.9726\n",
            "Epoch 8/10\n",
            "249/249 [==============================] - 3s 12ms/step - loss: 0.0945 - accuracy: 0.9640\n",
            "Epoch 9/10\n",
            "249/249 [==============================] - 4s 15ms/step - loss: 0.1083 - accuracy: 0.9600\n",
            "Epoch 10/10\n",
            "249/249 [==============================] - 3s 13ms/step - loss: 0.1266 - accuracy: 0.9583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Model Evaluation and Saving Model Weights\n"
      ],
      "metadata": {
        "id": "TWuiPOCP0Ozg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the dataset itself (for demonstration purposes)\n",
        "loss, accuracy = model.evaluate(dataset)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# Save the model weights\n",
        "#model.save_weights('name_gender_classifier_weights.h5')\n",
        "\n",
        "# Demonstrate loading the model weights (if needed in the future)\n",
        "model.load_weights('name_gender_classifier_weights.h5')\n",
        "\n",
        "# Predicting with a few sample names\n",
        "sample_names = np.array([\"Alice\", \"Bob\", \"Clarissa\", \"David\"])\n",
        "sample_labels = np.array([1, 0, 1, 0])  # Just as placeholders\n",
        "\n",
        "# Prepare sample data\n",
        "sample_dataset = tf.data.Dataset.from_tensor_slices((sample_names, sample_labels))\n",
        "sample_dataset = sample_dataset.map(vectorize_text)\n",
        "sample_dataset = sample_dataset.batch(32)\n",
        "\n",
        "# Making predictions\n",
        "for batch in sample_dataset:\n",
        "    predictions = model.predict(batch[0])\n",
        "    for name, prediction in zip(sample_names, predictions):\n",
        "        gender = \"Female\" if prediction > 0.5 else \"Male\"\n",
        "        print(f\"Name: {name}, Predicted Gender: {gender}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBOnz4AJwU8K",
        "outputId": "fe85f9d2-905c-4984-b65c-b12f8a31f62c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249/249 [==============================] - 2s 6ms/step - loss: 1.5667 - accuracy: 0.5003\n",
            "Loss:  1.5667062997817993\n",
            "Accuracy:  0.5002517700195312\n",
            "1/1 [==============================] - 0s 209ms/step\n",
            "Name: Alice, Predicted Gender: Male\n",
            "Name: Bob, Predicted Gender: Male\n",
            "Name: Clarissa, Predicted Gender: Male\n",
            "Name: David, Predicted Gender: Male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. LSTM Based Approach"
      ],
      "metadata": {
        "id": "yJ6Z1-DR0ZbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Data Preparation"
      ],
      "metadata": {
        "id": "0NEKF_DA4Sot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Download and prepare dataset\n",
        "nltk.download('names')\n",
        "from nltk.corpus import names\n",
        "\n",
        "# Load names\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "\n",
        "# Create labels\n",
        "male_labels = [0] * len(male_names)  # 0 for male\n",
        "female_labels = [1] * len(female_names)  # 1 for female\n",
        "\n",
        "# Combine datasets\n",
        "all_names = np.array(male_names + female_names)\n",
        "all_labels = np.array(male_labels + female_labels)\n",
        "\n",
        "# Shuffle dataset\n",
        "indices = np.arange(all_labels.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "all_names = all_names[indices]\n",
        "all_labels = all_labels[indices]\n",
        "\n",
        "# Text Vectorization\n",
        "vectorizer = TextVectorization(max_tokens=10000, output_sequence_length=10)\n",
        "vectorizer.adapt(all_names)\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "    return vectorizer(text), label\n",
        "\n",
        "# Prepare the final datasets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((all_names, all_labels))\n",
        "dataset = dataset.map(vectorize_text)\n",
        "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA74Igvaxvin",
        "outputId": "0b807a81-8e69-4308-db21-1482994627b7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Model Setup"
      ],
      "metadata": {
        "id": "Y17Zrlne4XAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=10),\n",
        "    LSTM(64),  # LSTM expects [batch, sequence, features]\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model again with corrections\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary to confirm architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9i0CoJkxxbd",
        "outputId": "8ae2ed35-b8a3-41d0-eef0-3c51844d0731"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 10, 64)            640000    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 677249 (2.58 MB)\n",
            "Trainable params: 677249 (2.58 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Training, Evaluation, and Saving the Model"
      ],
      "metadata": {
        "id": "8XFxZnTJ4ohA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "history = model.fit(dataset, epochs=10)\n",
        "\n",
        "# Evaluate the model on the same dataset\n",
        "# Note: Ideally, you should evaluate on a separate test set.\n",
        "loss, accuracy = model.evaluate(dataset)\n",
        "print(f\"Model Loss: {loss}, Model Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the entire model for future reference\n",
        "model.save('name_gender_classifier_lstm.h5')\n",
        "\n",
        "# Load the model (demonstrating loading, not necessary now since it's already in memory)\n",
        "loaded_model = tf.keras.models.load_model('name_gender_classifier_lstm.h5')\n",
        "\n",
        "# Making predictions with new data\n",
        "sample_names = np.array([\"Alice\", \"Bob\", \"Clarissa\", \"David\"])\n",
        "sample_labels = np.array([1, 0, 1, 0])  # Just placeholders for labels\n",
        "\n",
        "# Vectorize the sample names using the established vectorizer\n",
        "sample_dataset = tf.data.Dataset.from_tensor_slices((sample_names, sample_labels))\n",
        "sample_dataset = sample_dataset.map(vectorize_text)\n",
        "sample_dataset = sample_dataset.batch(32)\n",
        "\n",
        "# Predict using the loaded model\n",
        "predictions = loaded_model.predict(sample_dataset)\n",
        "for name, prediction in zip(sample_names, predictions):\n",
        "    gender = \"Female\" if prediction > 0.5 else \"Male\"\n",
        "    print(f\"Name: {name}, Predicted Gender: {gender}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZio8033yBD2",
        "outputId": "e8bebb36-6ce7-4a0f-ee44-cf91fc640cc6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "249/249 [==============================] - 5s 9ms/step - loss: 0.6608 - accuracy: 0.6295\n",
            "Epoch 2/10\n",
            "249/249 [==============================] - 2s 8ms/step - loss: 0.4969 - accuracy: 0.7450\n",
            "Epoch 3/10\n",
            "249/249 [==============================] - 3s 12ms/step - loss: 0.2279 - accuracy: 0.9330\n",
            "Epoch 4/10\n",
            "249/249 [==============================] - 2s 8ms/step - loss: 0.1658 - accuracy: 0.9517\n",
            "Epoch 5/10\n",
            "249/249 [==============================] - 2s 9ms/step - loss: 0.1701 - accuracy: 0.9510\n",
            "Epoch 6/10\n",
            "249/249 [==============================] - 2s 8ms/step - loss: 0.1706 - accuracy: 0.9496\n",
            "Epoch 7/10\n",
            "249/249 [==============================] - 3s 11ms/step - loss: 0.1657 - accuracy: 0.9485\n",
            "Epoch 8/10\n",
            "249/249 [==============================] - 2s 9ms/step - loss: 0.1215 - accuracy: 0.9552\n",
            "Epoch 9/10\n",
            "249/249 [==============================] - 2s 8ms/step - loss: 0.1033 - accuracy: 0.9563\n",
            "Epoch 10/10\n",
            "249/249 [==============================] - 2s 8ms/step - loss: 0.0999 - accuracy: 0.9569\n",
            "249/249 [==============================] - 2s 6ms/step - loss: 0.1204 - accuracy: 0.9537\n",
            "Model Loss: 0.12037023901939392, Model Accuracy: 0.9536757469177246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 625ms/step\n",
            "Name: Alice, Predicted Gender: Female\n",
            "Name: Bob, Predicted Gender: Male\n",
            "Name: Clarissa, Predicted Gender: Female\n",
            "Name: David, Predicted Gender: Male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. https://arxiv.org/pdf/2102.03692.pdf\n",
        "2. https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/13-attention.html\n",
        "3. https://towardsdatascience.com/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044\n",
        "4. https://www.nltk.org/book/ch02.html#sec-lexical-resources"
      ],
      "metadata": {
        "id": "ExMITGgCdQWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHiDsdXLhbbW"
      }
    }
  ]
}